## [Naver Finance Financials Crawler] ChatGPT, Cursor AI를 통한 바이브 코딩
개발 기간 : 2026.01.06 ~ 2026.01.07 
<br>
<br>
네이버 증권에서 **재무 요약 테이블(연간 Y / 분기 Q)** 을 크롤링한 뒤, 결과를 **엑셀(.xlsx)** 로 저장하는 스크립트

- 입력: CSV (각 행: **종목코드, 종목명, 시장구분**)
- 출력: Excel (`output/financials_result.xlsx`)
  - `RESULT` 시트: 성공 종목 데이터
  - `FAIL` 시트: 실패 종목 및 에러 사유
 
  
## 1) 프로젝트 구조
stock/ <br>
├─ main.py <br>
├─ prepare_tickers.py <br>
├─ requirements.txt <br>
├─ data/ <br>
│ └─ tickers_filtered.csv <br> 
├─ crawler/ <br>
│ └─ financials.py <br>
│ └─ exporter.py <br>
│ └─ tickers.py <br>
└─ output/ <br>
&nbsp; &nbsp;└─ financials_result.xlsx

## 2) 동작 방식

아래의 KRX Data Marketplace 에서 전종목 csv 파일 다운로드 <br>
https://data.krx.co.kr/contents/MDC/MDI/mdiLoader/index.cmd?menuId=MDC0201020101

다운로드 받은 csv 파일을 `krx_data` 로 파일명 변경 후 루트 디렉토리 단계에 넣기

1. `prepare_tickers.py` 실행
2. 전종목 리스트 csv를 읽음
3. 시장구분이 `KOSPI`, `KOSDAQ` 인 종목, 네이버 증권에 존재하는 종목만 필터링
4. `data/tickers_filtered.csv`를 생성
5.  `main.py` 실행 
6. 각 종목에 대해:
   - 연간(Y), 분기(Q) 재무 테이블을 가져옴
   - 결산월 차이를 없애기 위해 기간 컬럼을 정규화
     - 연간: `2024/03` → `2024`
     - 분기: `2024/06` → `2024Q2`
   - 같은 카테고리(예: 매출액)는 **연간(Y) 컬럼을 먼저 나열하고, 그 다음 분기(Q) 컬럼을 나열**하도록 정렬 **(아빠의 요구사항)**
7. 전체 종목 루프가 끝난 뒤 엑셀 저장

---

## 3) 입력 CSV 형식

`data/tickers_filtered.csv` 예시:

```csv
종목코드,종목명,시장구분
005930,삼성전자,KOSPI
035420,NAVER,KOSPI
068270,셀트리온,KOSPI
```

## 4) Environment
- Python 3.9+
- Google Chrome 설치 필요

## 5) Setup
```bash
python -m venv venv
.\venv\Scripts\Activate.ps1
pip install -r requirements.txt
```

## 6. Step
필터링 로직 실행
```
python prepare_tickers.py
```
메인 로직 실행
```
python main.py
```
`output/financial_result.xlsl` 확인

## 7. AI를 활용한 바이브 코딩, 하지만 2일이면 오래 걸린 것 아닌가?

개발자를 꿈꾸며 교육을 받는 동안, 종종 아빠의 주식 정보 크롤링 요청이 들어왔었다. <br>
SSAFY를 수료하고, 본격적인 취업을 위해 포트폴리오와 이력서를 정리했다. <br>
10개의 서류를 지원하면, 1곳 정도에서는 면접 연락이 오는 단계 <br>
여기서 만족해서는 안되지만, 지금이 아빠의 1년 동안의 꾸준한 요청을 들어줄 수 있는 적기라고 생각했다. 
<br><br> 
그동안 프로젝트를 하면서:
- 직접 코드를 구현하며
- AI의 코드가 왜 이런 코드인지
- 내가 짠 코드와 무엇이 다른지를 비교하면서
- 무조건적인 AI 맹신, AI에 잡아먹히는 개발자가 되지 않게끔 노력했다. <br>

### 하지만 이번에는 AI의 도움을 적극적으로 받고
- AI에게 개선 사항과
- 오류 로그를 제공 <br>
하는 정도만 하면 솔직히 하루 안에 끝날 줄 알았다.

재무제표 그대로 가져오는 것이 아닌, 연간 탭의 재무제표와 분기 탭의 제무재표를 크롤링 해야했기 때문에 단순히 url 과 재무제표 테이블을 찾는다고 해서 끝나지 않았다.

전체 재무제표(“전체” 탭)를 긁는 게 아니라 ‘연간’ 탭(Y) / ‘분기’ 탭(Q) 의 값이 각각 별도로 존재해서, 단순 HTML 파싱으로는 끝나지 않았다. “전체” 화면의 표는 일부 항목이 빠져 있거나 값이 달라서, 필요한 데이터는 결국 탭 클릭 시 발생하는 AJAX 요청을 그대로 재현해야 했다.

처음에는 `finance.naver.com/item/coinfo.naver?code=...` 같은 페이지에서 테이블을 찾는 방식으로 접근했는데, 여기서는 “연간/분기” 탭의 데이터가 **정적 HTML로 노출되지 않고**, 탭을 누를 때마다 **비동기(AJAX)** 로 별도 테이블이 내려오는 구조였다. 즉 “표를 찾는다”는 목표는 같았지만, 표가 HTML에 이미 존재하는 게 아니라 **요청을 통해 새로 받아와야 하는 데이터였다.**

그래서 해결 방향을 “테이블 파싱”이 아니라 “탭 클릭 시 요청 재현”으로 바꿨다. 개발자 도구(Network)를 기준으로 확인해보면, 연간(Y)과 분기(Q) 탭을 누를 때 각각 아래와 같은 형태의 요청이 발생하는 것을 알 수 있었다.

- 연간(Y): `.../ajax/cF1001.aspx?cmp_cd=...&fin_typ=...&freq_typ=Y&encparam=...&id=...`

- 분기(Q): `.../ajax/cF1001.aspx?cmp_cd=...&fin_typ=...&freq_typ=Q&encparam=...&id=...`

여기서 핵심은 `encparam`과 `id` 였다. 이 두 파라미터는 아무 값이 아니라, 종목마다 **동적으로 생성되는 인증/세션 성격의 값**이라서 단순히 `cmp_cd(종목코드)`만 바꿔 호출하면 데이터가 내려오지 않았다. 

즉, “AJAX URL을 알았다”는 것만으로는 부족했고, 그 **URL을 구성하기 위한 선행 단계**가 필요했다.

그래서 다음과 같은 흐름으로 구성했다.

### 1. 종목별로 `encparam`과 `id`를 먼저 확보

`c1010001.aspx?cmp_cd=종목코드&target=finsum_more` 페이지를 먼저 요청한다.

HTML/스크립트 안에서 정규식을 이용해 `encparam`과 `id` 값을 추출한다.

이 값이 있어야만 이후 `cF1001.aspx` AJAX 호출이 “정상 응답”을 준다.

### 2. 연간(Y) / 분기(Q) AJAX 요청을 각각 호출

확보한 `encparam`, `id`를 붙여서 `freq_typ=Y`와 `freq_typ=Q` 를 각각 호출한다.

응답 HTML에서 `pandas.read_html()`로 테이블을 파싱한다.

### 3. 진짜 재무 테이블만 선별

`read_html()`이 반환하는 테이블에는 의미 없는 값(예: 9393 같은 상수 도배)이나 부가 정보 테이블이 섞여 있었다.

그래서 `매출액/영업이익/EPS/ROE …` 같은 키워드 존재 여부, 숫자 셀 개수, 상수 도배 여부 등을 점수화해서 재무 테이블만 골라내는 필터 로직을 추가했다.

“페이지에서 표를 찾아 파싱한다” 에서 끝나는 게 아니라,


**(1) 연간 / 분기 탭 데이터는 AJAX로 내려온다**

**(2) AJAX 호출엔 동적 파라미터가 필요하다**

**(3) 선행 페이지에서 파라미터를 추출해야 한다** 

라는 단계를 진행하면서 시간을 많이 쏟았다.

그동안은 AI가 준 코드를 “그대로 가져다 쓰는 방식”을 일부러 피했다.
내 코드와 AI 코드가 왜 다른지, 무엇이 더 낫고 무엇이 위험한지를 비교하면서, AI를 맹신하지 않는 습관을 유지해왔다.

그런데 이번 프로젝트에서는 처음으로 AI의 도움을 적극적으로 받는 방식, 즉 사실상 바이브 코딩을 본격적으로 경험했다.
속도는 확실히 빨라졌지만, **동시에 더 강하게 깨달은 게 있다.**

### AI는 절대 만능이 아니다.

맞는 말도 하지만, 상황(사이트 차단/세션/예외/데이터 구조/엣지케이스)에 따라 틀린 방향으로 가거나 “그럴듯한 코드”를 만들기도 한다. <br>
이에 속지 않고 올바른 방향으로 사용자가 이끌어줘야 한다.

### 바이브 코딩도 '기본기'가 있어야 효과가 난다.

요청을 정확히 쪼개서 던지고, 결과를 검증하고, 로그를 읽고, 실패 원인을 좁혀나가는 능력(HTTP, 파싱, 데이터 정규화, 예외처리)이 있어야
AI가 만든 코드를 내 프로젝트에 맞게 수정·통제할 수 있다. <br>
AI가 준 코드를 파악하지 않고 그대로 붙이는 순간, 그때부터 AI에게 잡아먹히는 개발자가 된다.




  























